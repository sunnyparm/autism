import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse, parse_qs
import re
import sqlite3
import urllib3
import ssl
from urllib3.poolmanager import PoolManager
from requests.adapters import HTTPAdapter
try:
    import chardet  # optional
except Exception:
    chardet = None

# --- í…”ë ˆê·¸ë¨ ì„¤ì • (ì‚¬ìš©ì ì œê³µ ì •ë³´) ---
TELEGRAM_BOT_TOKEN = '6250305411:AAHWIpJDIUU57x_cFORKGsPwecQq_QYlWmw'
TELEGRAM_CHAT_ID = 752516623

# í…”ë ˆê·¸ë¨ ì„¤ì • í™•ì¸
def check_telegram_config():
    """í…”ë ˆê·¸ë¨ ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸"""
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("âš ï¸  í…”ë ˆê·¸ë¨ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤!")
        print("1. í…”ë ˆê·¸ë¨ì—ì„œ @BotFatherì™€ ëŒ€í™”í•˜ì—¬ ë´‡ ìƒì„±")
        print("2. ë´‡ í† í°ê³¼ ì±„íŒ… IDë¥¼ ì½”ë“œì— ì„¤ì •")
        print("3. ìì„¸í•œ ë°©ë²•ì€ 'í…”ë ˆê·¸ë¨_ì„¤ì •_ê°€ì´ë“œ.md' íŒŒì¼ ì°¸ì¡°")
        return False
    print(f"âœ… í…”ë ˆê·¸ë¨ ì„¤ì • í™•ì¸ë¨ - ë´‡ í† í°: {TELEGRAM_BOT_TOKEN[:10]}..., ì±„íŒ… ID: {TELEGRAM_CHAT_ID}")
    return True

DB_PATH = 'autismnews.db'
TEST_MODE = False  # ì‹¤ì œ ëª¨ë“œ: í…”ë ˆê·¸ë¨ ì „ì†¡ í™œì„±í™”
DEBUG_LIST_ALL = False  # ë””ë²„ê·¸: ë‚ ì§œ í•„í„°ì™€ ë¬´ê´€í•˜ê²Œ ëª¨ë“  í–‰ì˜ ì œëª©/ì¼ìë¥¼ ì¶œë ¥
DAYS_WINDOW = 5  # ìµœê·¼ 5ì¼ ì´ë‚´ë§Œ ìˆ˜ì§‘
HEADER_TITLE = 'í•œêµ­ìíì¸ì‚¬ë‘í˜‘íšŒ ê¸°ì‚¬'

def init_db():
    if TEST_MODE:
        return
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS sent_news (
                title TEXT,
                date TEXT,
                PRIMARY KEY (title, date)
            )
        """)
        conn.commit()

def is_already_sent(title, date):
    if TEST_MODE:
        return False
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.cursor()
        cur.execute("SELECT 1 FROM sent_news WHERE title = ? AND date = ?", (title, date))
        return cur.fetchone() is not None

def save_sent(title, date):
    if TEST_MODE:
        return
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.cursor()
        try:
            cur.execute("INSERT INTO sent_news (title, date) VALUES (?, ?)", (title, date))
            conn.commit()
        except sqlite3.IntegrityError:
            pass

def send_telegram_message(message):
    """í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    if TEST_MODE:
        print("í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í…”ë ˆê·¸ë¨ ì „ì†¡ ê±´ë„ˆëœ€")
        return False
        
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("í…”ë ˆê·¸ë¨ ë´‡ í† í° ë˜ëŠ” chat_idê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ì „ì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # telepot ëŒ€ì‹  requestsë¥¼ ì‚¬ìš©í•˜ì—¬ í…”ë ˆê·¸ë¨ API ì§ì ‘ í˜¸ì¶œ
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        
        # ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ë©´ ë¶„í• 
        max_length = 4096
        messages = [message[i:i+max_length] for i in range(0, len(message), max_length)]
        
        for msg_part in messages:
            if msg_part.strip():
                data = {
                    'chat_id': TELEGRAM_CHAT_ID,
                    'text': msg_part,
                    'parse_mode': 'HTML'
                }
                response = requests.post(url, data=data, timeout=10)
                if response.status_code != 200:
                    print(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                    return False
        
        print("í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ.")
        return True
        
    except Exception as e:
        print(f"ì˜¤ë¥˜: í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# ==============================
# ìˆ˜ì§‘ ëŒ€ìƒ URL (ì¹´í…Œê³ ë¦¬ë³„)
# ==============================
urls = [
    ('https://www.autismkorea.kr/bbs/board.php?tbl=bbs31', 'ê³µì§€ì‚¬í•­'),
    ('https://www.autismkorea.kr/bbs/board.php?tbl=bbs36', 'ë‰´ìŠ¤ë ˆí„°'),
    ('https://www.autismkorea.kr/bbs/board.php?tbl=bbs32', 'ì–¸ë¡ ë³´ë„'),
    ('https://www.autismkorea.kr/bbs/board.php?tbl=bbs34', 'ì™¸ë¶€ê¸°ê´€ ì†Œì‹')
]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

# SSL ê²€ì¦ ê²½ê³  ë¹„í™œì„±í™” (verify=False ì‚¬ìš© ì‹œ)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class TLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        ctx = ssl.create_default_context()
        # SSL ê²€ì¦ ì™„ì „ ë¹„í™œì„±í™”
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        
        # ë‹¤ì–‘í•œ SSL ì„¤ì • ì‹œë„
        try:
            # ë³´ì•ˆ ë ˆë²¨ì„ ìµœì†Œë¡œ ì„¤ì •
            ctx.set_ciphers('DEFAULT@SECLEVEL=0')
        except ssl.SSLError:
            try:
                # ëŒ€ì²´ ë°©ë²•: ëª¨ë“  ì•”í˜¸í™” ë°©ì‹ í—ˆìš©
                ctx.set_ciphers('ALL:!aNULL:!eNULL')
            except ssl.SSLError:
                pass
        
        # í”„ë¡œí† ì½œ ë²„ì „ ì„¤ì •
        try:
            ctx.minimum_version = ssl.TLSVersion.TLSv1
            ctx.maximum_version = ssl.TLSVersion.TLSv1_3
        except AttributeError:
            # êµ¬ë²„ì „ Python í˜¸í™˜ì„±
            try:
                ctx.protocol = ssl.PROTOCOL_TLS
            except:
                pass
        
        self.poolmanager = PoolManager(*args, ssl_context=ctx, **kwargs)

# ì—¬ëŸ¬ SSL ì„¤ì • ë°©ë²• ì‹œë„
def create_session_with_ssl_fallback():
    """SSL ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì—¬ëŸ¬ ë°©ë²•ì„ ì‹œë„í•˜ëŠ” ì„¸ì…˜ ìƒì„±"""
    
    # ë°©ë²• 1: ê°œì„ ëœ TLSAdapter ì‚¬ìš©
    try:
        session = requests.Session()
        session.mount('https://', TLSAdapter())
        return session
    except Exception as e:
        print(f"TLSAdapter ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 2: ê¸°ë³¸ ì„¸ì…˜ì— SSL ë¹„í™œì„±í™”
    try:
        session = requests.Session()
        session.verify = False
        return session
    except Exception as e:
        print(f"ê¸°ë³¸ ì„¸ì…˜ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 3: urllib3 ì§ì ‘ ì‚¬ìš©
    try:
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        session = requests.Session()
        session.verify = False
        return session
    except Exception as e:
        print(f"urllib3 ì„¸ì…˜ ì‹¤íŒ¨: {e}")
    
    # ìµœí›„ì˜ ìˆ˜ë‹¨: ê¸°ë³¸ requests
    return requests.Session()

session = create_session_with_ssl_fallback()

# ìµœê·¼ 5ì¼ ì´ë‚´ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
five_days_ago = datetime.now() - timedelta(days=DAYS_WINDOW)
msg = ''
count = 1
debug_count = 1  # DEBUG_LIST_ALL ì¶œë ¥ìš© ìˆœë²ˆ

# í…”ë ˆê·¸ë¨ ì„¤ì • í™•ì¸
if not check_telegram_config():
    print("í…”ë ˆê·¸ë¨ ì„¤ì • í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit(1)

init_db()

def make_request_with_retry(url, max_retries=3):
    """SSL ì˜¤ë¥˜ë¥¼ í¬í•¨í•œ ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ìš”ì²­ í•¨ìˆ˜"""
    for attempt in range(max_retries):
        print(f"ìš”ì²­ ì‹œì‘ (ì‹œë„ {attempt + 1}/{max_retries}): {url}")
        
        try:
            # ë‹¤ì–‘í•œ SSL ì„¤ì •ìœ¼ë¡œ ì‹œë„
            if attempt == 0:
                # ì²« ë²ˆì§¸ ì‹œë„: ê¸°ë³¸ ì„¤ì •
                res = session.get(url, headers=headers, timeout=15, verify=False)
            elif attempt == 1:
                # ë‘ ë²ˆì§¸ ì‹œë„: ë‹¤ë¥¸ SSL ì„¤ì •
                import ssl
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
                res = session.get(url, headers=headers, timeout=15, verify=False)
            else:
                # ì„¸ ë²ˆì§¸ ì‹œë„: requests ì§ì ‘ ì‚¬ìš©
                res = requests.get(url, headers=headers, timeout=15, verify=False)
            
            print(f"ìƒíƒœ ì½”ë“œ: {res.status_code}, ì‘ë‹µ ê¸¸ì´: {len(res.content)}")
            return res
            
        except ssl.SSLError as e:
            print(f"SSL ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                print("ë‹¤ë¥¸ SSL ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...")
                continue
            else:
                print("ëª¨ë“  SSL ì„¤ì • ì‹œë„ ì‹¤íŒ¨")
                return None
        except Exception as e:
            print(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                print("ì¬ì‹œë„...")
                continue
            else:
                print("ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
                return None
    
    return None

# ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ì§‘
categorized_articles = {}

for url, category in urls:
    print(f"\n=== {category} ìˆ˜ì§‘ ì¤‘ ===")
    res = make_request_with_retry(url)
    if res is None:
        continue
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    if category not in categorized_articles:
        categorized_articles[category] = []
    # ì‘ë‹µ ì¸ì½”ë”© íŒë³„: meta > headers > chardet > cp949 ë°±ì—…
    raw = res.content
    enc_candidates = []
    # meta charset ì¶”ì¶œ
    try:
        head_snippet = raw[:4096].decode('ascii', errors='ignore')
        m = re.search(r'charset=([\w\-]+)', head_snippet, re.IGNORECASE)
        if m:
            enc_candidates.append(m.group(1).lower())
    except Exception:
        pass
    if res.encoding:
        enc_candidates.append(res.encoding.lower())
    if chardet is not None:
        try:
            detected = chardet.detect(raw).get('encoding')
            if detected:
                enc_candidates.append(detected.lower())
        except Exception:
            pass
    # í•œêµ­ ì‚¬ì´íŠ¸ ì¼ë°˜ ë°±ì—… ì¸ì½”ë”©
    enc_candidates += ['utf-8', 'cp949', 'euc-kr']
    # í›„ë³´ ì¤‘ í•œê¸€ ê²€ì¶œë˜ëŠ” ì²« ë””ì½”ë”© ì‚¬ìš©
    selected = None
    for enc in enc_candidates:
        try:
            trial = raw.decode(enc, errors='replace')
            if re.search(r'[\uac00-\ud7a3]', trial):
                selected = enc
                html = trial
                break
        except Exception:
            continue
    if selected is None:
        # ë§ˆì§€ë§‰ ë°±ì—…
        html = raw.decode('utf-8', errors='replace')
    soup = BeautifulSoup(html, 'html.parser')

    # -----------------------------
    # (1) í•œêµ­ìíì¸ì‚¬ë‘í˜‘íšŒ ê³µì§€ì‚¬í•­
    # -----------------------------
    table = soup.find('table', class_='basic_board_list')
    if not table:
        # Fallback: ì¹´ë“œí˜• ëª©ë¡ (ul.horizontal_board > li)
        items = soup.select('ul.horizontal_board li')
        if not items:
            print("ëª©ë¡ í…Œì´ë¸”/ì¹´ë“œ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue
        print(f"ì¹´ë“œí˜• ëª©ë¡ ê°œìˆ˜: {len(items)}")
        for li in items:
            a_tag = li.select_one('div.txt_box h4 a') or li.select_one('h4 a')
            if not a_tag:
                continue
            title = a_tag.get_text(strip=True)
            link = a_tag.get('href')
            full_url = urljoin(url, link)
            # VIEW ë§í¬ ì •ê·œí™”
            try:
                parsed = urlparse(full_url)
                qs = parse_qs(parsed.query)
                num = qs.get('num', [None])[0]
                tbl = qs.get('tbl', [''])[0]
                if num:
                    base = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                    if not tbl:
                        # ì›ë³¸ URLì˜ tbl ìœ ì§€ ì‹œë„
                        orig_qs = parse_qs(urlparse(url).query)
                        tbl = orig_qs.get('tbl', [''])[0]
                    if tbl:
                        full_url = f"{base}?tbl={tbl}&mode=VIEW&num={num}"
            except Exception:
                pass

            # ë‚ ì§œ ì¶”ì¶œ: 'ì‘ì„±ì¼' ì£¼ë³€ ë˜ëŠ” YYYY-MM-DD íŒ¨í„´ ì°¾ê¸°
            date_key = None
            date_obj = None
            em = li.select_one('em')
            date_text = em.get_text(" ", strip=True) if em else li.get_text(" ", strip=True)
            m = re.search(r'(20\d{2}-\d{2}-\d{2})', date_text)
            if m:
                date_key = m.group(1)
                try:
                    y, mo, d = map(int, date_key.split('-'))
                    date_obj = datetime(y, mo, d)
                except Exception:
                    date_obj = None

            if not date_key or not date_obj:
                # ë‹¤ë¥¸ í¬ë§· ì‹œë„: YY.MM.DD
                m2 = re.search(r'(\d{2})\.(\d{2})\.(\d{2})', date_text)
                if m2:
                    y = int('20' + m2.group(1))
                    mo = int(m2.group(2))
                    d = int(m2.group(3))
                    try:
                        date_obj = datetime(y, mo, d)
                        date_key = date_obj.strftime('%Y-%m-%d')
                    except Exception:
                        date_key = None

            if not date_key:
                # ë‚ ì§œ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue

            if DEBUG_LIST_ALL:
                print(f"{debug_count}\t{title}\t{date_key}\t{full_url}")
                debug_count += 1

            # ìµœê·¼ 5ì¼ ì´ë‚´ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
            if date_obj >= five_days_ago:
                if not is_already_sent(title, date_key):
                    article_info = {
                        'title': title,
                        'date': date_key,
                        'url': full_url
                    }
                    categorized_articles[category].append(article_info)
                    print(f"{len(categorized_articles[category])}. {title} ({date_key})")
                    save_sent(title, date_key)
        # ì¹´ë“œí˜•ì„ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ í…Œì´ë¸” íŒŒì‹±ì€ ê±´ë„ˆëœ€
        continue
    rows = table.find_all('tr')

    print(f"í–‰ ê°œìˆ˜: {len(rows)}")
    for row in rows:
        td_left = row.find('td', class_='left')
        if not td_left:
            continue

        a_tag = td_left.find('a')
        if not a_tag:
            continue

        title = a_tag.text.strip()
        link = a_tag.get('href')
        full_url = urljoin(url, link)
        # ê¹”ë”í•œ VIEW ë§í¬ë¡œ ì •ê·œí™”: ...&mode=VIEW&num=XXXX
        try:
            parsed = urlparse(full_url)
            qs = parse_qs(parsed.query)
            num = qs.get('num', [None])[0]
            tbl = qs.get('tbl', ['bbs31'])[0]
            if num:
                base = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                full_url = f"{base}?tbl={tbl}&mode=VIEW&num={num}"
        except Exception:
            pass

        # ë‚ ì§œëŠ” <span> ì¤‘ ë‘ ë²ˆì§¸ (ì˜ˆ: 25.09.11)
        spans = td_left.find_all('span')
        if len(spans) >= 2:
            date_str = spans[1].text.strip()
        else:
            date_str = None

        if not date_str:
            continue

        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        try:
            # '25.09.11' â†’ '2025-09-11'
            year = int('20' + date_str.split('.')[0])
            month = int(date_str.split('.')[1])
            day = int(date_str.split('.')[2])
            date_obj = datetime(year, month, day)
            date_key = date_obj.strftime('%Y-%m-%d')
        except Exception:
            continue

        if DEBUG_LIST_ALL:
            print(f"{debug_count}\t{title}\t{date_key}\t{full_url}")
            debug_count += 1

        # ìµœê·¼ 5ì¼ ì´ë‚´ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
        if date_obj >= five_days_ago:
            if not is_already_sent(title, date_key):
                article_info = {
                    'title': title,
                    'date': date_key,
                    'url': full_url
                }
                categorized_articles[category].append(article_info)
                print(f"{len(categorized_articles[category])}. {title} ({date_key})")
                save_sent(title, date_key)

# ==============================
# ì¹´í…Œê³ ë¦¬ë³„ í…”ë ˆê·¸ë¨ ì „ì†¡
# ==============================
total_articles = sum(len(articles) for articles in categorized_articles.values())

if total_articles > 0:
    print(f"\n=== ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ===")
    
    for category, articles in categorized_articles.items():
        if not articles:
            continue
            
        print(f"\n--- {category} ({len(articles)}ê°œ) ---")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë©”ì‹œì§€ ìƒì„±
        category_message = f"<b>ğŸ“¢ {category}</b>\n\n"
        
        for i, article in enumerate(articles, 1):
            # HTML íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
            title = article['title'].replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
            article_text = f"<b>{i}.</b> {title}\nğŸ“… {article['date']}\nğŸ”— <a href='{article['url']}'>ë§í¬</a>\n\n"
            
            # ë©”ì‹œì§€ ê¸¸ì´ í™•ì¸ (4000ì ì œí•œ)
            if len(category_message + article_text) > 4000:
                # í˜„ì¬ ë©”ì‹œì§€ ì „ì†¡
                print(f"ì „ì†¡í•  ë©”ì‹œì§€ ({category}):\n{category_message}")
                if not TEST_MODE:
                    send_telegram_message(category_message)
                else:
                    print("í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í…”ë ˆê·¸ë¨ ì „ì†¡ ê±´ë„ˆëœ€")
                
                # ìƒˆ ë©”ì‹œì§€ ì‹œì‘
                category_message = f"<b>ğŸ“¢ {category} (ê³„ì†)</b>\n\n" + article_text
            else:
                category_message += article_text
        
        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì „ì†¡
        if category_message.strip():
            print(f"ì „ì†¡í•  ë©”ì‹œì§€ ({category}):\n{category_message}")
            if not TEST_MODE:
                send_telegram_message(category_message)
            else:
                print("í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í…”ë ˆê·¸ë¨ ì „ì†¡ ê±´ë„ˆëœ€")
                
else:
    # ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ëŠ” ê²½ìš°
    no_news_message = f"<b>{HEADER_TITLE}</b>\n\nì‹ ê·œ ê³µì§€ì‚¬í•­ ì—†ìŒ"
    print(no_news_message)
    
    if not TEST_MODE:
        send_telegram_message(no_news_message)
    else:
        print("í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í…”ë ˆê·¸ë¨ ì „ì†¡ ê±´ë„ˆëœ€")
